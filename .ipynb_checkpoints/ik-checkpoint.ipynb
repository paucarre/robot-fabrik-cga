{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import tqdm\n",
    "from tensordict.nn import TensorDictModule\n",
    "from tensordict.tensordict import TensorDict, TensorDictBase\n",
    "from torch import nn\n",
    "\n",
    "from torchrl.data import BoundedTensorSpec, CompositeSpec, UnboundedContinuousTensorSpec\n",
    "from torchrl.envs import (\n",
    "    CatTensors,\n",
    "    EnvBase,\n",
    "    Transform,\n",
    "    TransformedEnv,\n",
    "    UnsqueezeTransform,\n",
    ")\n",
    "from torchrl.envs.transforms.transforms import _apply_to_composite\n",
    "from torchrl.envs.utils import check_env_specs, step_mdp\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "thetas_count = 6\n",
    "pose_count = 6\n",
    "error_done_threshold = 1e-3\n",
    "weights = torch.Tensor([1.0, 0.0, 0.0, 0.0, 0.0, 0.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from pytorch3d import transforms\n",
    "import math\n",
    "from linguamechanica.kinematics import DifferentiableOpenChainMechanism\n",
    "from linguamechanica.kinematics import UrdfRobotLibrary\n",
    "\n",
    "urdf_robot = UrdfRobotLibrary.dobot_cr5()\n",
    "open_chain = urdf_robot.extract_open_chains(0.3)[-1]\n",
    "\n",
    "def force_parameters_within_bounds(thetas):\n",
    "    bigger_than_pi = thetas > math.pi\n",
    "    thetas[bigger_than_pi] = thetas[bigger_than_pi] - (2.0 * math.pi)\n",
    "    less_than_minus_pi = thetas < -math.pi\n",
    "    thetas[less_than_minus_pi] = thetas[less_than_minus_pi] + (2.0 * math.pi)\n",
    "    return thetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_reward(thetas, target_pose, weights, error_done_threshold):\n",
    "    if len(thetas.shape) == 1:\n",
    "        thetas = thetas.unsqueeze(0)\n",
    "    if len(target_pose.shape) == 1:\n",
    "        target_pose = target_pose.unsqueeze(0)\n",
    "    error_pose = open_chain.compute_error_pose(\n",
    "        thetas, target_pose\n",
    "    )\n",
    "    pose_error = DifferentiableOpenChainMechanism.compute_weighted_error(\n",
    "        error_pose, weights\n",
    "    )\n",
    "    done = pose_error < error_done_threshold\n",
    "    reward = - pose_error\n",
    "    return reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def _step(tensordict):\n",
    "    thetas = tensordict[\"thetas\"]\n",
    "    theta_deltas = tensordict[\"action\"]\n",
    "    max_theta_deltas = tensordict[\"params\", \"max_theta_deltas\"]\n",
    "    print(thetas.shape, theta_deltas.shape)\n",
    "    new_thetas = thetas - theta_deltas\n",
    "    new_thetas = new_thetas.clamp(-max_theta_deltas, max_theta_deltas)\n",
    "    target_pose = tensordict[\"target_pose\"]\n",
    "    #TODO: I have no idea if this is a good idea or not\n",
    "    new_thetas = force_parameters_within_bounds(new_thetas)\n",
    "    reward, done = compute_reward(new_thetas, target_pose, weights, error_done_threshold)\n",
    "    out = TensorDict(\n",
    "        {\n",
    "            \"next\": {\n",
    "                \"thetas\": new_thetas,\n",
    "                \"target_pose\": target_pose,\n",
    "                \"params\": tensordict[\"params\"],\n",
    "                \"reward\": reward,\n",
    "                \"done\": done,\n",
    "            }\n",
    "        },\n",
    "        tensordict.shape,\n",
    "    )\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniformly_sample_parameters_within_constraints(open_chain, batch_size):\n",
    "    samples = []\n",
    "    for sample_idx in range(batch_size):\n",
    "        coordinates = []\n",
    "        for i in range(len(open_chain.joint_limits)):\n",
    "            # TODO: check if unconstrained works\n",
    "            coordinates.append(\n",
    "                random.uniform(\n",
    "                    open_chain.joint_limits[i][0],\n",
    "                    open_chain.joint_limits[i][1],\n",
    "                )\n",
    "            )\n",
    "        samples.append(torch.Tensor(coordinates).unsqueeze(0))\n",
    "    return torch.cat(samples, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_target_pose(target_thetas):\n",
    "    if len(target_thetas.shape) == 1:\n",
    "        target_thetas = target_thetas.unsqueeze(0)\n",
    "    target_transformation = open_chain.forward_transformation(\n",
    "        target_thetas\n",
    "    )\n",
    "    target_pose = transforms.se3_log_map(\n",
    "        target_transformation.get_matrix()\n",
    "    )\n",
    "    if target_thetas.shape[0] == 1:\n",
    "        target_thetas = target_thetas.squeeze(0)\n",
    "    return target_pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def _reset(self, tensordict):\n",
    "    if tensordict is None or tensordict.is_empty():\n",
    "        # if no tensordict is passed, we generate a single set of hyperparameters\n",
    "        # Otherwise, we assume that the input tensordict contains all the relevant\n",
    "        # parameters to get started.\n",
    "        tensordict = self.gen_params(batch_size=self.batch_size)\n",
    "    batch_size = 1 if len(tensordict.shape) == 0 else tensordict.shape[0]\n",
    "    thetas = uniformly_sample_parameters_within_constraints(open_chain, batch_size).to(device=self.device)\n",
    "    if batch_size == 1:\n",
    "        thetas = thetas.squeeze(0)\n",
    "    thetas = force_parameters_within_bounds(thetas)\n",
    "    #TODO: randommize this better\n",
    "    target_thetas = thetas + torch.randn(thetas.shape)\n",
    "    target_thetas = force_parameters_within_bounds(target_thetas)\n",
    "    target_pose =  generate_random_target_pose(target_thetas)\n",
    "    #TODO: finish this\n",
    "    target_pose = (\n",
    "        torch.rand([*tensordict.shape, pose_count], generator=self.rng, device=self.device)\n",
    "    )\n",
    "    out = TensorDict(\n",
    "        {\n",
    "            \"thetas\": thetas,\n",
    "            \"target_pose\": target_pose,\n",
    "            \"params\": tensordict[\"params\"],\n",
    "        },\n",
    "        batch_size=tensordict.shape,\n",
    "    )\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def _make_spec(self, td_params):\n",
    "    # Under the hood, this will populate self.output_spec[\"observation\"]\n",
    "    self.observation_spec = CompositeSpec(\n",
    "        thetas=BoundedTensorSpec(\n",
    "            minimum=-torch.ones(thetas_count) * torch.pi,\n",
    "            maximum= torch.ones(thetas_count) * torch.pi,\n",
    "            shape=(thetas_count),\n",
    "            dtype=torch.float32,\n",
    "        ),\n",
    "        #TODO: bounds are wrong\n",
    "        target_pose=BoundedTensorSpec(\n",
    "            minimum=-torch.ones(thetas_count) * torch.pi,\n",
    "            maximum= torch.ones(thetas_count) * torch.pi,\n",
    "            shape=(pose_count),\n",
    "            dtype=torch.float32,\n",
    "        ),\n",
    "        # we need to add the \"params\" to the observation specs, as we want\n",
    "        # to pass it at each step during a rollout\n",
    "        params=make_composite_from_td(td_params[\"params\"]),\n",
    "        shape=(),\n",
    "    )\n",
    "    # since the environment is stateless, we expect the previous output as input.\n",
    "    # For this, EnvBase expects some state_spec to be available\n",
    "    self.state_spec = self.observation_spec.clone()\n",
    "    # action-spec will be automatically wrapped in input_spec when\n",
    "    # `self.action_spec = spec` will be called supported\n",
    "    self.action_spec = BoundedTensorSpec(\n",
    "        minimum=-td_params[\"params\", \"max_theta_deltas\"],\n",
    "        maximum=td_params[\"params\", \"max_theta_deltas\"],\n",
    "        shape=(thetas_count),\n",
    "        dtype=torch.float32,\n",
    "    )\n",
    "    self.reward_spec = UnboundedContinuousTensorSpec(shape=(*td_params.shape, 1))\n",
    "\n",
    "\n",
    "def make_composite_from_td(td):\n",
    "    # custom funtion to convert a tensordict in a similar spec structure\n",
    "    # of unbounded values.\n",
    "    composite = CompositeSpec(\n",
    "        {\n",
    "            key: make_composite_from_td(tensor)\n",
    "            if isinstance(tensor, TensorDictBase)\n",
    "            else UnboundedContinuousTensorSpec(\n",
    "                dtype=tensor.dtype, device=tensor.device, shape=tensor.shape\n",
    "            )\n",
    "            for key, tensor in td.items()\n",
    "        },\n",
    "        shape=td.shape,\n",
    "    )\n",
    "    return composite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def _set_seed(self, seed: Optional[int]):\n",
    "    rng = torch.manual_seed(seed)\n",
    "    self.rng = rng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def gen_params(g=10.0, batch_size=None) -> TensorDictBase:\n",
    "    \"\"\"Returns a tensordict containing the physical parameters such as gravitational force and torque or speed limits.\"\"\"\n",
    "    if batch_size is None:\n",
    "        batch_size = []\n",
    "    td = TensorDict(\n",
    "        {\n",
    "            \"params\": TensorDict(\n",
    "                {\n",
    "                    \"max_theta_deltas\": torch.ones(thetas_count) * 0.1,\n",
    "                },\n",
    "                [],\n",
    "            )\n",
    "        },\n",
    "        [],\n",
    "    )\n",
    "    if batch_size:\n",
    "        td = td.expand(batch_size).contiguous()\n",
    "    return td"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class InverseKinematicsEnv(EnvBase):\n",
    "    metadata = {\n",
    "        \"render_modes\": [\"human\", \"rgb_array\"],\n",
    "        \"render_fps\": 30,\n",
    "    }\n",
    "    batch_locked = False\n",
    "\n",
    "    def __init__(self, td_params=None, seed=None, device=\"cpu\"):\n",
    "        if td_params is None:\n",
    "            td_params = self.gen_params()\n",
    "\n",
    "        super().__init__(device=device, batch_size=[])\n",
    "        self._make_spec(td_params)\n",
    "        if seed is None:\n",
    "            seed = torch.empty((), dtype=torch.int64).random_().item()\n",
    "        self.set_seed(seed)\n",
    "\n",
    "    # Helpers: _make_step and gen_params\n",
    "    gen_params = staticmethod(gen_params)\n",
    "    _make_spec = _make_spec\n",
    "\n",
    "    # Mandatory methods: _step, _reset and _set_seed\n",
    "    _reset = _reset\n",
    "    _step = staticmethod(_step)\n",
    "    _set_seed = _set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "check_env_specs succeeded!\n"
     ]
    }
   ],
   "source": [
    "env = InverseKinematicsEnv()\n",
    "check_env_specs(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can have a look at our specs to have a visual representation of the environment\n",
    "signature:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation_spec: CompositeSpec(\n",
      "    thetas: BoundedTensorSpec(\n",
      "        shape=torch.Size([6]),\n",
      "        space=ContinuousBox(\n",
      "            minimum=Tensor(shape=torch.Size([6]), device=cpu, dtype=torch.float32, contiguous=True), \n",
      "            maximum=Tensor(shape=torch.Size([6]), device=cpu, dtype=torch.float32, contiguous=True)),\n",
      "        device=cpu,\n",
      "        dtype=torch.float32,\n",
      "        domain=continuous),\n",
      "    target_pose: BoundedTensorSpec(\n",
      "        shape=torch.Size([6]),\n",
      "        space=ContinuousBox(\n",
      "            minimum=Tensor(shape=torch.Size([6]), device=cpu, dtype=torch.float32, contiguous=True), \n",
      "            maximum=Tensor(shape=torch.Size([6]), device=cpu, dtype=torch.float32, contiguous=True)),\n",
      "        device=cpu,\n",
      "        dtype=torch.float32,\n",
      "        domain=continuous),\n",
      "    params: CompositeSpec(\n",
      "        max_theta_deltas: UnboundedContinuousTensorSpec(\n",
      "            shape=torch.Size([6]),\n",
      "            space=None,\n",
      "            device=cpu,\n",
      "            dtype=torch.float32,\n",
      "            domain=continuous), device=cpu, shape=torch.Size([])), device=cpu, shape=torch.Size([]))\n",
      "state_spec: CompositeSpec(\n",
      "    thetas: BoundedTensorSpec(\n",
      "        shape=torch.Size([6]),\n",
      "        space=ContinuousBox(\n",
      "            minimum=Tensor(shape=torch.Size([6]), device=cpu, dtype=torch.float32, contiguous=True), \n",
      "            maximum=Tensor(shape=torch.Size([6]), device=cpu, dtype=torch.float32, contiguous=True)),\n",
      "        device=cpu,\n",
      "        dtype=torch.float32,\n",
      "        domain=continuous),\n",
      "    target_pose: BoundedTensorSpec(\n",
      "        shape=torch.Size([6]),\n",
      "        space=ContinuousBox(\n",
      "            minimum=Tensor(shape=torch.Size([6]), device=cpu, dtype=torch.float32, contiguous=True), \n",
      "            maximum=Tensor(shape=torch.Size([6]), device=cpu, dtype=torch.float32, contiguous=True)),\n",
      "        device=cpu,\n",
      "        dtype=torch.float32,\n",
      "        domain=continuous),\n",
      "    params: CompositeSpec(\n",
      "        max_theta_deltas: UnboundedContinuousTensorSpec(\n",
      "            shape=torch.Size([6]),\n",
      "            space=None,\n",
      "            device=cpu,\n",
      "            dtype=torch.float32,\n",
      "            domain=continuous), device=cpu, shape=torch.Size([])), device=cpu, shape=torch.Size([]))\n",
      "reward_spec: UnboundedContinuousTensorSpec(\n",
      "    shape=torch.Size([1]),\n",
      "    space=ContinuousBox(\n",
      "        minimum=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, contiguous=True), \n",
      "        maximum=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, contiguous=True)),\n",
      "    device=cpu,\n",
      "    dtype=torch.float32,\n",
      "    domain=continuous)\n"
     ]
    }
   ],
   "source": [
    "print(\"observation_spec:\", env.observation_spec)\n",
    "print(\"state_spec:\", env.state_spec)\n",
    "print(\"reward_spec:\", env.reward_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can execute a couple of commands too to check that the output structure\n",
    "matches what is expected.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reset tensordict TensorDict(\n",
      "    fields={\n",
      "        done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        params: TensorDict(\n",
      "            fields={\n",
      "                max_theta_deltas: Tensor(shape=torch.Size([6]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "            batch_size=torch.Size([]),\n",
      "            device=cpu,\n",
      "            is_shared=False),\n",
      "        target_pose: Tensor(shape=torch.Size([6]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        thetas: Tensor(shape=torch.Size([6]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "    batch_size=torch.Size([]),\n",
      "    device=cpu,\n",
      "    is_shared=False)\n"
     ]
    }
   ],
   "source": [
    "td = env.reset()\n",
    "print(\"reset tensordict\", td)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run the :func:`env.rand_step` to generate\n",
    "an action randomly from the ``action_spec`` domain. A tensordict containing\n",
    "the hyperparams and the current state **must** be passed since our\n",
    "environment is stateless. In stateful contexts, ``env.rand_step()`` works\n",
    "perfectly too.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6]) torch.Size([6])\n",
      "random step tensordict TensorDict(\n",
      "    fields={\n",
      "        action: Tensor(shape=torch.Size([6]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        next: TensorDict(\n",
      "            fields={\n",
      "                done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                params: TensorDict(\n",
      "                    fields={\n",
      "                        max_theta_deltas: Tensor(shape=torch.Size([6]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "                    batch_size=torch.Size([]),\n",
      "                    device=cpu,\n",
      "                    is_shared=False),\n",
      "                reward: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                target_pose: Tensor(shape=torch.Size([6]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                thetas: Tensor(shape=torch.Size([6]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "            batch_size=torch.Size([]),\n",
      "            device=cpu,\n",
      "            is_shared=False),\n",
      "        params: TensorDict(\n",
      "            fields={\n",
      "                max_theta_deltas: Tensor(shape=torch.Size([6]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "            batch_size=torch.Size([]),\n",
      "            device=cpu,\n",
      "            is_shared=False),\n",
      "        target_pose: Tensor(shape=torch.Size([6]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        thetas: Tensor(shape=torch.Size([6]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "    batch_size=torch.Size([]),\n",
      "    device=cpu,\n",
      "    is_shared=False)\n"
     ]
    }
   ],
   "source": [
    "td = env.rand_step(td)\n",
    "print(\"random step tensordict\", td)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_env = TransformedEnv(\n",
    "    env,\n",
    "    # Unsqueezes the observations that we will concatenate\n",
    "    UnsqueezeTransform(\n",
    "        unsqueeze_dim=-1,\n",
    "        in_keys=[\"thetas\", \"target_pose\"],\n",
    "        in_keys_inv=[\"thetas\", \"target_pose\"],\n",
    "    ),\n",
    ")\n",
    "class OnManifodError(Transform):\n",
    "    def _apply_transform(self, obs: torch.Tensor) -> None:\n",
    "        print(\"!!!\", obs)\n",
    "        return obs.sin()\n",
    "\n",
    "    # _apply_to_composite will execute the observation spec transform across all\n",
    "    # in_keys/out_keys pairs and write the result in the observation_spec which\n",
    "    # is of type ``Composite``\n",
    "    @_apply_to_composite\n",
    "    def transform_observation_spec(self, observation_spec):\n",
    "        return BoundedTensorSpec(\n",
    "            minimum=-1,\n",
    "            maximum=1,\n",
    "            shape=(6),\n",
    "            dtype=observation_spec.dtype,\n",
    "            device=observation_spec.device,\n",
    "        )\n",
    "\n",
    "\n",
    "on_manifold_error = SinTransform(in_keys=[\"thetas\", \"target_pose\"], out_keys=[\"on_manifold_error\"])\n",
    "transformed_env.append_transform(on_manifold_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#cat_transform = CatTensors(\n",
    "#    in_keys=[\"sin\", \"cos\", \"thdot\"], dim=-1, out_key=\"observation\", del_keys=False\n",
    "#)\n",
    "#transformed_env.append_transform(cat_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once more, let us check that our env specs match what is received:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "check_env_specs succeeded!\n"
     ]
    }
   ],
   "source": [
    "check_env_specs(transformed_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executing a rollout\n",
    "\n",
    "Executing a rollout is a succession of simple steps:\n",
    "\n",
    "* reset the environment\n",
    "* while some condition is not met:\n",
    "\n",
    "  * compute an action given a policy\n",
    "  * execute a step given this action\n",
    "  * collect the data\n",
    "  * make a MDP step\n",
    "\n",
    "* gather the data and return\n",
    "\n",
    "These operations have been convinently wrapped in the :func:`EnvBase.rollout`\n",
    "method, from which we provide a simplified version here below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "data from rollout: TensorDict(\n",
      "    fields={\n",
      "        action: Tensor(shape=torch.Size([100, 6]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        done: Tensor(shape=torch.Size([100, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        next: TensorDict(\n",
      "            fields={\n",
      "                done: Tensor(shape=torch.Size([100, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                on_manifold_error: Tensor(shape=torch.Size([100, 6, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                params: TensorDict(\n",
      "                    fields={\n",
      "                        max_theta_deltas: Tensor(shape=torch.Size([100, 6]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "                    batch_size=torch.Size([100]),\n",
      "                    device=None,\n",
      "                    is_shared=False),\n",
      "                reward: Tensor(shape=torch.Size([100, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                target_pose: Tensor(shape=torch.Size([100, 6, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                thetas: Tensor(shape=torch.Size([100, 6, 1]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "            batch_size=torch.Size([100]),\n",
      "            device=None,\n",
      "            is_shared=False),\n",
      "        on_manifold_error: Tensor(shape=torch.Size([100, 6, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        params: TensorDict(\n",
      "            fields={\n",
      "                max_theta_deltas: Tensor(shape=torch.Size([100, 6]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "            batch_size=torch.Size([100]),\n",
      "            device=None,\n",
      "            is_shared=False),\n",
      "        target_pose: Tensor(shape=torch.Size([100, 6, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        thetas: Tensor(shape=torch.Size([100, 6, 1]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "    batch_size=torch.Size([100]),\n",
      "    device=None,\n",
      "    is_shared=False)\n"
     ]
    }
   ],
   "source": [
    "def simple_rollout(steps=100):\n",
    "    # preallocate:\n",
    "    data = TensorDict({}, [steps])\n",
    "    # reset\n",
    "    _data = transformed_env.reset()\n",
    "    for i in range(steps):\n",
    "        _data[\"action\"] = transformed_env.action_spec.rand()\n",
    "        _data = transformed_env.step(_data)\n",
    "        data[i] = _data\n",
    "        _data = step_mdp(_data, keep_other=True)\n",
    "    return data\n",
    "\n",
    "\n",
    "print(\"data from rollout:\", simple_rollout(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batching computations\n",
    "\n",
    "The last unexplored end of our tutorial is the ability that we have to\n",
    "batch computations in TorchRL. Because our environment does not\n",
    "make any assumptions regarding the input data shape, we can seamlessly\n",
    "execute it over batches of data. Even better: for non-batch-locked\n",
    "environments such as our Pendulum, we can change the batch size on the fly\n",
    "without recreating the env.\n",
    "To do this, we just generate parameters with the desired shape.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reset (batch size of 10) TensorDict(\n",
      "    fields={\n",
      "        done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        on_manifold_error: Tensor(shape=torch.Size([10, 6, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        params: TensorDict(\n",
      "            fields={\n",
      "                max_theta_deltas: Tensor(shape=torch.Size([10, 6]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "            batch_size=torch.Size([10]),\n",
      "            device=None,\n",
      "            is_shared=False),\n",
      "        target_pose: Tensor(shape=torch.Size([10, 6, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        thetas: Tensor(shape=torch.Size([10, 6, 1]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "    batch_size=torch.Size([10]),\n",
      "    device=None,\n",
      "    is_shared=False)\n",
      "torch.Size([10, 6]) torch.Size([10, 6])\n",
      "rand step (batch size of 10) TensorDict(\n",
      "    fields={\n",
      "        action: Tensor(shape=torch.Size([10, 6]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        next: TensorDict(\n",
      "            fields={\n",
      "                done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                on_manifold_error: Tensor(shape=torch.Size([10, 6, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                params: TensorDict(\n",
      "                    fields={\n",
      "                        max_theta_deltas: Tensor(shape=torch.Size([10, 6]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "                    batch_size=torch.Size([10]),\n",
      "                    device=None,\n",
      "                    is_shared=False),\n",
      "                reward: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                target_pose: Tensor(shape=torch.Size([10, 6, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                thetas: Tensor(shape=torch.Size([10, 6, 1]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "            batch_size=torch.Size([10]),\n",
      "            device=None,\n",
      "            is_shared=False),\n",
      "        on_manifold_error: Tensor(shape=torch.Size([10, 6, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        params: TensorDict(\n",
      "            fields={\n",
      "                max_theta_deltas: Tensor(shape=torch.Size([10, 6]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "            batch_size=torch.Size([10]),\n",
      "            device=None,\n",
      "            is_shared=False),\n",
      "        target_pose: Tensor(shape=torch.Size([10, 6, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        thetas: Tensor(shape=torch.Size([10, 6, 1]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "    batch_size=torch.Size([10]),\n",
      "    device=None,\n",
      "    is_shared=False)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10  # number of environments to be executed in batch\n",
    "td = transformed_env.reset(transformed_env.gen_params(batch_size=[batch_size]))\n",
    "print(\"reset (batch size of 10)\", td)\n",
    "td = transformed_env.rand_step(td)\n",
    "print(\"rand step (batch size of 10)\", td)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "executing a rollout with a batch of data requires us to reset the env\n",
    "out of the rollout function, since we need to define the batch_size\n",
    "dynamically and this is not supported by :func:`EnvBase.rollout`:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 6]) torch.Size([10, 6])\n",
      "rollout of len 3 (batch size of 10): TensorDict(\n",
      "    fields={\n",
      "        action: Tensor(shape=torch.Size([10, 1, 6]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        done: Tensor(shape=torch.Size([10, 1, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        next: TensorDict(\n",
      "            fields={\n",
      "                done: Tensor(shape=torch.Size([10, 1, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                on_manifold_error: Tensor(shape=torch.Size([10, 1, 6, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                params: TensorDict(\n",
      "                    fields={\n",
      "                        max_theta_deltas: Tensor(shape=torch.Size([10, 1, 6]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "                    batch_size=torch.Size([10, 1]),\n",
      "                    device=None,\n",
      "                    is_shared=False),\n",
      "                reward: Tensor(shape=torch.Size([10, 1, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                target_pose: Tensor(shape=torch.Size([10, 1, 6, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                thetas: Tensor(shape=torch.Size([10, 1, 6, 1]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "            batch_size=torch.Size([10, 1]),\n",
      "            device=None,\n",
      "            is_shared=False),\n",
      "        on_manifold_error: Tensor(shape=torch.Size([10, 1, 6, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        params: TensorDict(\n",
      "            fields={\n",
      "                max_theta_deltas: Tensor(shape=torch.Size([10, 1, 6]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "            batch_size=torch.Size([10, 1]),\n",
      "            device=None,\n",
      "            is_shared=False),\n",
      "        target_pose: Tensor(shape=torch.Size([10, 1, 6, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        thetas: Tensor(shape=torch.Size([10, 1, 6, 1]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "    batch_size=torch.Size([10, 1]),\n",
      "    device=None,\n",
      "    is_shared=False)\n"
     ]
    }
   ],
   "source": [
    "rollout = transformed_env.rollout(\n",
    "    3,\n",
    "    auto_reset=False,  # we're executing the reset out of the ``rollout`` call\n",
    "    tensordict=transformed_env.reset(transformed_env.gen_params(batch_size=[batch_size])),\n",
    ")\n",
    "print(\"rollout of len 3 (batch size of 10):\", rollout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a simple policy\n",
    "\n",
    "In this example, we will train a simple policy using the reward as a\n",
    "differentiable objective (i.e. a negative loss).\n",
    "We will take advantage of the fact that our dynamic system is fully\n",
    "differentiable to backpropagate through the trajectory return and adjust the\n",
    "weights of our policy to maximise this value directly. Of course, in many\n",
    "settings many of the assumptions we make do not hold, such as\n",
    "differentiability of the system and full access to the underlying mechanics.\n",
    "\n",
    "Still, this is a very simple example that showcases how a training loop can\n",
    "be coded with a custom environment in TorchRL.\n",
    "\n",
    "Let us first write the policy network:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "transformed_env.set_seed(0)\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.LazyLinear(64),\n",
    "    nn.Tanh(),\n",
    "    nn.LazyLinear(64),\n",
    "    nn.Tanh(),\n",
    "    nn.LazyLinear(64),\n",
    "    nn.Tanh(),\n",
    "    nn.LazyLinear(thetas_count),\n",
    ")\n",
    "policy = TensorDictModule(\n",
    "    net,\n",
    "    in_keys=[\"on_manifold_error\"],\n",
    "    out_keys=[\"action\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and our optimizer:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam(policy.parameters(), lr=2e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop\n",
    "\n",
    "We will successively:\n",
    "\n",
    "* generate a trajectory\n",
    "* sum the rewards\n",
    "* backpropagate through the graph defined by these operations\n",
    "* clip the gradient norm and make an optimization step\n",
    "* repeat\n",
    "\n",
    "At the end of the training loop, we should have a final reward close to 0\n",
    "which demonstrates that the pendulum is upward and still as desired.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/625 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 6]) torch.Size([32, 6, 6])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (32) must match the size of tensor b (6) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[446], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[1;32m      7\u001b[0m     init_td \u001b[38;5;241m=\u001b[39m transformed_env\u001b[38;5;241m.\u001b[39mreset(transformed_env\u001b[38;5;241m.\u001b[39mgen_params(batch_size\u001b[38;5;241m=\u001b[39m[batch_size]))\n\u001b[0;32m----> 8\u001b[0m     rollout \u001b[38;5;241m=\u001b[39m \u001b[43mtransformed_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensordict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_td\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauto_reset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     traj_return \u001b[38;5;241m=\u001b[39m rollout[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnext\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreward\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m     10\u001b[0m     (\u001b[38;5;241m-\u001b[39mtraj_return)\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/linguamechanica-XFzcHPK6-py3.10/lib/python3.10/site-packages/torchrl/envs/common.py:1238\u001b[0m, in \u001b[0;36mEnvBase.rollout\u001b[0;34m(self, max_steps, policy, callback, auto_reset, auto_cast_to_device, break_when_any_done, return_contiguous, tensordict)\u001b[0m\n\u001b[1;32m   1236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m auto_cast_to_device:\n\u001b[1;32m   1237\u001b[0m     tensordict \u001b[38;5;241m=\u001b[39m tensordict\u001b[38;5;241m.\u001b[39mto(env_device, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m-> 1238\u001b[0m tensordict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensordict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1240\u001b[0m tensordicts\u001b[38;5;241m.\u001b[39mappend(tensordict\u001b[38;5;241m.\u001b[39mclone(\u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[1;32m   1241\u001b[0m done \u001b[38;5;241m=\u001b[39m tensordict\u001b[38;5;241m.\u001b[39mget((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnext\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone_key))\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/linguamechanica-XFzcHPK6-py3.10/lib/python3.10/site-packages/torchrl/envs/common.py:822\u001b[0m, in \u001b[0;36mEnvBase.step\u001b[0;34m(self, tensordict)\u001b[0m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;66;03m# sanity check\u001b[39;00m\n\u001b[1;32m    820\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assert_tensordict_shape(tensordict)\n\u001b[0;32m--> 822\u001b[0m tensordict_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensordict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;66;03m# this tensordict should contain a \"next\" key\u001b[39;00m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/linguamechanica-XFzcHPK6-py3.10/lib/python3.10/site-packages/torchrl/envs/transforms/transforms.py:631\u001b[0m, in \u001b[0;36mTransformedEnv._step\u001b[0;34m(self, tensordict)\u001b[0m\n\u001b[1;32m    629\u001b[0m tensordict \u001b[38;5;241m=\u001b[39m tensordict\u001b[38;5;241m.\u001b[39mclone(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    630\u001b[0m tensordict_in \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform\u001b[38;5;241m.\u001b[39minv(tensordict)\n\u001b[0;32m--> 631\u001b[0m tensordict_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensordict_in\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;66;03m# we want the input entries to remain unchanged\u001b[39;00m\n\u001b[1;32m    633\u001b[0m tensordict_out \u001b[38;5;241m=\u001b[39m tensordict\u001b[38;5;241m.\u001b[39mupdate(tensordict_out)\n",
      "Cell \u001b[0;32mIn[411], line 8\u001b[0m, in \u001b[0;36m_step\u001b[0;34m(tensordict)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#if len(thetas.shape) == 2:\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#    max_theta_deltas = max_theta_deltas.unsqueeze(1)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(thetas\u001b[38;5;241m.\u001b[39mshape, theta_deltas\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m----> 8\u001b[0m new_thetas \u001b[38;5;241m=\u001b[39m \u001b[43mthetas\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtheta_deltas\u001b[49m\n\u001b[1;32m      9\u001b[0m new_thetas \u001b[38;5;241m=\u001b[39m new_thetas\u001b[38;5;241m.\u001b[39mclamp(\u001b[38;5;241m-\u001b[39mmax_theta_deltas, max_theta_deltas)\n\u001b[1;32m     10\u001b[0m target_pose \u001b[38;5;241m=\u001b[39m tensordict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget_pose\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (32) must match the size of tensor b (6) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "pbar = tqdm.tqdm(range(20_000 // batch_size))\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, 20_000)\n",
    "logs = defaultdict(list)\n",
    "\n",
    "for _ in pbar:\n",
    "    init_td = transformed_env.reset(transformed_env.gen_params(batch_size=[batch_size]))\n",
    "    rollout = transformed_env.rollout(100, policy, tensordict=init_td, auto_reset=False)\n",
    "    traj_return = rollout[\"next\", \"reward\"].mean()\n",
    "    (-traj_return).backward()\n",
    "    gn = torch.nn.utils.clip_grad_norm_(net.parameters(), 1.0)\n",
    "    optim.step()\n",
    "    optim.zero_grad()\n",
    "    pbar.set_description(\n",
    "        f\"reward: {traj_return: 4.4f}, \"\n",
    "        f\"last reward: {rollout[..., -1]['next', 'reward'].mean(): 4.4f}, gradient norm: {gn: 4.4}\"\n",
    "    )\n",
    "    logs[\"return\"].append(traj_return.item())\n",
    "    logs[\"last_reward\"].append(rollout[..., -1][\"next\", \"reward\"].mean().item())\n",
    "    scheduler.step()\n",
    "\n",
    "\n",
    "def plot():\n",
    "    import matplotlib\n",
    "    from matplotlib import pyplot as plt\n",
    "\n",
    "    is_ipython = \"inline\" in matplotlib.get_backend()\n",
    "    if is_ipython:\n",
    "        from IPython import display\n",
    "\n",
    "    with plt.ion():\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(logs[\"return\"])\n",
    "        plt.title(\"returns\")\n",
    "        plt.xlabel(\"iteration\")\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(logs[\"last_reward\"])\n",
    "        plt.title(\"last reward\")\n",
    "        plt.xlabel(\"iteration\")\n",
    "        if is_ipython:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we have learned how to code a stateless environment from\n",
    "scratch. We touched the subjects of:\n",
    "\n",
    "* the four essential components that need to be taken care of when coding\n",
    "  an environment (:func:`step`, :func:`reset\", seeding and building specs).\n",
    "  We saw how these methods and classes interact with the\n",
    "  :class:`tensordict.TensorDict` class;\n",
    "* how to test that an environment is properly coded using\n",
    "  :func:`~torchrl.envs.utils.check_env_specs`;\n",
    "* How to append transforms in the context of stateless environments and how\n",
    "  to write custom transformations;\n",
    "* How to train a policy on a fully differentiable simulator.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
